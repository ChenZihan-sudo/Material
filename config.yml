Default:
  absolute_work_dir: /home/catcolia/Material
  device: cuda
  result_path: results
  tune_result_path: tune_results
  dataset_dir: dataset

# Task
Process:
  raw_dir: "{{Default.absolute_work_dir}}/{{Default.dataset_dir}}/raw"
  use_ase_atoms_raw_data: true
  processed_dir: "{{Default.absolute_work_dir}}/{{Default.dataset_dir}}/processed"
  max_cutoff_distance: 8.0
  max_cutoff_neighbors: 100
  edge:
    normalization: true
    edge_feature: 30
    gaussian_smearing:
      enable: true
      # resolution: "{{Process.edge.edge_feature}}" # deprecated
      width: 0.2
  # auto generate processed dataset name
  auto_processed_name: true

# Task
Training:
  epochs: 1000
  dataset:
    trainset_ratio: 0.8
    valset_ratio: 0.1
    testset_ratio: 0.1
    seed: 1
  data_loader:
    batch_size: 800
    num_workers: 1
    seed: 1
  save_best_model: true # get best model based on best test loss
  # save_step: 2 # save results every save_step epochs
  save_result_on: "{{Default.absolute_work_dir}}/{{Default.result_path}}"
  # load model from a checkpoint file, empty (blank) here will train from begin
  load_model_from: # "{{Default.absolute_work_dir}}/results/ChemGNN/1724537109002412"
  # resume training will restore all status and cover the original checkpoint file
  resume_training: true

# Task
Tuning:
  storage_path: "{{Default.absolute_work_dir}}/{{Default.tune_result_path}}/tune"
  log_to_file: true # set true will enable the log trainable output
  dataset:
    trainset_ratio: 0.8
    valset_ratio: 0.1
    testset_ratio: 0.1
    seed: 1
    # delete processed dataset after end of final epoch or reach `keep_best_epochs`
    delete_after_completion: true

  data_loader:
    batch_size: 100
    num_workers: 1
    seed: 1

  # data will start saving if the model loss reaches this value
  save_loss_limit: 1.0
  # the maximum trials (how many trials the experiment can sample)
  # If this is -1, (virtually) infinite samples are generated until a stopping condition is met.
  trial_num_samples: -1
  # a training task (a trial) will run less or equal to the `max_epochs` epochs
  max_epochs: 1100 # 1000
  # the whole experiment will stop after `time_budget_s` seconds
  time_budget_s:

  # max concurrent trials if paralleled,
  # set this according to your task and hardware resources
  max_concurrent_trials: 10
  resources:
    num_cpus: 16 # total hardware cpu(thread) resources
    num_gpus: 1 # total hardware gpu resources
    trial_cpus: 8 # a trail uses trial_cpus=num_cpus/concurrent_trials if paralleled
    trial_gpus: 0.5 # a trial uses trial_gpus=num_gpus/concurrent_trials if paralleled

  # the trial will end if the best model does not appear after the `keep_best_epochs` epochs,
  # the counter will reset to 0 when the best model appears
  # every epoch keep_best_epochs + 1 if the best model does not appear.
  keep_best_epochs: 100
  save_result_on: "{{Default.absolute_work_dir}}/{{Default.tune_result_path}}"

  # set true will load or process dataset from trainable model method
  load_dataset_by_trainable: true

  # restore experiment from a experiment path, empty (blank) here will start training from begin
  restore_experiment_from: "{{Tuning.storage_path}}/ChemGNN_MPDatasetCeCoCuBased_2024-09-10_23-20-07"

# Task
# Analysis:

Dataset:
  MPDataset: # Material Project
    raw_dir: "{{Process.raw_dir}}/mp"
    processed_dir: "{{Process.processed_dir}}/mp"
    download:
      exclude_elements: ["O"]
      include_elements:
      api_key: j61NN3yuDh8tQWf0OrkachbbUoJ8npVP
      chunk_size: 1000
      num_elements: [3, 3]
      num_chunks:
      keep_data_from: "{{Dataset.MPDataset.raw_dir}}/origin_INDICES" # keep the data sorting from the INDICES file
    onehot_gen: false
    onehot_range: [1, 101]
    get_parameters_from: "{{Dataset.MPDataset.processed_dir}}"
  MPDatasetLarge:
    processed_filename: CONFIG
    total_data_num: 41819
    processed_dir: "{{Process.processed_dir}}/mpl"
    get_parameters_from: "{{Dataset.MPDataset.processed_dir}}"
  MPDatasetCeCoCuBased:
    raw_dir: "{{Process.raw_dir}}/mpb"
    processed_dir: "{{Process.processed_dir}}/mpb"
    download:
      exclude_elements: ["O"]
      include_elements: ["Ce", "Co", "Cu"]
      api_key: j61NN3yuDh8tQWf0OrkachbbUoJ8npVP
      chunk_size: 1000
      num_elements: [3, 3]
      num_chunks:
      keep_data_from: # keep the data sorting from the INDICES file
    onehot_gen: false
    onehot_range: [1, 101]
    get_parameters_from: "{{Dataset.MPDatasetCeCoCuBased.processed_dir}}"
  HypoDataset: # Unoptimized Hypothesis Compounds
    processed_dir: "{{Process.processed_dir}}/hypo"
    scales: [0.96, 0.98, 1.00, 1.02, 1.04]
    atomic_numbers: [58, 27, 29]
    processed_filename: data # processed data filename
    processed_ase_filename: ase_data
    split_num: 20 # split the dataset into multiple data block
    get_parameters_from: "{{Dataset.MPDataset.raw_dir}}"
  UnoptimizedHypoDataset: # Unoptimized Hypothesis Compounds (Sampled)
    raw_dir: "{{Process.raw_dir}}/unopt_hypo"
    processed_dir: "{{Process.processed_dir}}/unopt_hypo"
    onehot_range: [1, 101]
    total_num: 4542
    get_parameters_from: "{{Dataset.MPDataset.raw_dir}}"
  OptimizedHypoDataset: # Optimized Hypothesis Compounds (Sampled)
    raw_dir: "{{Process.raw_dir}}/opt_hypo"
    processed_dir: "{{Process.processed_dir}}/opt_hypo"
    onehot_range: [1, 101]
    total_num: 4542
    formation_energy_filename: FORMATION_ENERGY_
    compound_filename: POSCAR_
    inherit_parameters_from: "{{Dataset.MPDataset.raw_dir}}"
    get_parameters_from: "{{Dataset.OptimizedHypoDataset.inherit_parameters_from}}"

Models:
  ChemGNN:
    conv_params:
      aggregators: [sum, mean, min, max, std]
      scalers: [identity]
      # edge_dim: "{{Process.edge.edge_feature}}" # deprecated, use Process.edge.edge_feature instead
      towers: 1
      pre_layers: 1
      post_layers: 1
      divide_input: false
      aggMLP: false
      aggMLP_factor: 0.5
    pre_fc_dim: [100]
    num_layers: 1
    conv_out_dim: 100
    post_fc_dim: [150, 100]
    dropout_rate: 0.0
    pool: "global_mean_pool"
    learning_rate: 0.02
    optimizer:
      name: AdamW
      params:
    scheduler:
      name: ReduceLROnPlateau
      params:
        mode: min
        factor: 0.85
        patience: 10
        min_lr: 0.00000001
  PNA:
    conv_params:
      aggregators: [sum, mean, min, max, std]
      scalers: [identity, amplification, attenuation]
      # edge_dim: "{{Process.edge.edge_feature}}" # deprecated, use Process.edge.edge_feature instead
      towers: 1
      pre_layers: 1
      post_layers: 1
      divide_input: false
    pre_fc_dim: [100]
    num_layers: 1
    conv_out_dim: 100
    post_fc_dim: [150, 150]
    dropout_rate: 0.3
    pool: "global_mean_pool"
    learning_rate: 0.01
    optimizer:
      name: AdamW
      params:
    scheduler:
      name: ReduceLROnPlateau
      params:
        mode: min
        factor: 0.8
        patience: 30
        min_lr: 0.00000001
  GCN:
    conv_params:
      improved: true
    pre_fc_dim: [200, 200]
    num_layers: 2
    conv_out_dim: 100
    post_fc_dim: [200, 200]
    dropout_rate: 0.6
    pool: "global_mean_pool"
    learning_rate: 0.01
    optimizer:
      name: AdamW
      params:
    scheduler:
      name: ReduceLROnPlateau
      params:
        mode: min
        factor: 0.8
        patience: 30
        min_lr: 0.00000001

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/material/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "\n",
    "from dataset import make_dataset\n",
    "from train import make_data_loader, train_step, test_evaluations, save_model_GCN\n",
    "from utils import get_device, plot_training_progress\n",
    "from model import GCNNetwork, CEALNetwork\n",
    "\n",
    "from args import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, validation_dataset, test_dataset = make_dataset()\n",
    "train_loader, val_loader, test_loader = make_data_loader(train_dataset, validation_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"CEAL\"\n",
    "model_network = model_name + \"Network\"\n",
    "model_args = args[model_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29274 6273 6272\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset), len(validation_dataset), len(test_dataset))\n",
    "\n",
    "device = get_device()\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "in_dim = train_dataset[0].x.shape[-1]\n",
    "deg = generate_deg(train_dataset).float()\n",
    "deg = deg.to(device)\n",
    "model = CEALNetwork(deg, in_dim)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=model_args[\"learning_rate\"])\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=model_args[\"sche_mode\"], factor=model_args[\"sche_factor\"], patience=model_args[\"sche_patience\"], min_lr=model_args[\"sche_min_lr\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAr4AAAIjCAYAAADlfxjoAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAA5EklEQVR4nO3deVSV1f7H8Q/jQZwFBTQStcxZSy9EamZhZF5Ts6T0KnJNryldk1+DOKFZ4ny5vzLN2UanVTZoKpJUlkWpmDleM9NMcIrwigLK8/uj5fl1AgfwwAH3+7UWa3X22fs53+d8oz4+7vMcN8uyLAEAAAA3OHdXFwAAAACUBYIvAAAAjEDwBQAAgBEIvgAAADACwRcAAABGIPgCAADACARfAAAAGIHgCwAAACMQfAEAAGAEgi8AlDNLliyRm5ubvv32W6ced+DAgQoJCXHqMUvzuOWFm5ubJkyYUKK1ISEhGjhwoFPrAVByBF/gBlVa4elGcOm9udzPV1995eoScRVX6+Glnxs5kAMoPk9XFwAArvLCCy+oQYMGhcZvueUWF1RTcc2fP18FBQVl+pp333233njjDYexJ554QqGhoRoyZIh9rEqVKtf9WufOnZOnZ8n+d7lv3z65u3ONCSgvCL4AjNW1a1e1a9fO1WVUWGfPnlXlypXl5eVV5q/dsGFDNWzY0GFs6NChatiwof72t79ddt2FCxdUUFAgb2/va34tHx+fEtdps9lKvBaA8/HHUMBw27dvV9euXVWtWjVVqVJF9913X6G/6s/Pz9fEiRN16623ysfHR35+furQoYOSk5PtczIyMhQTE6ObbrpJNptNQUFB6tGjhw4dOnTZ154xY4bc3Nz0008/FXouPj5e3t7e+vXXXyVJ//nPf9S7d28FBgbKx8dHN910kx577DH99ttvznkjinDo0CG5ublpxowZ+te//qX69eurUqVK6tSpk77//vtC8z/55BN17NhRlStXVo0aNdSjRw/t2bOn0LyjR49q0KBBqlu3rmw2mxo0aKAnn3xSeXl5DvNyc3MVFxen2rVrq3LlyurVq5dOnDhxTbWvXr1aLVq0kI+Pj1q0aKH33nuv0JzU1FS5ubkpNTW1yPNesmSJfWzgwIGqUqWKfvjhBz344IOqWrWq+vXrZ3/uj1sK/vi+zZs3T40aNZLNZtNf/vIXffPNN4XqWLlypZo1a+ZQqzP2Df+xjqSkJHsdu3fvVl5ensaPH6+2bduqevXqqly5sjp27KhNmzYVOs6f9/hOmDBBbm5uOnDggAYOHKgaNWqoevXqiomJUU5OjsPaP+/xvbRF44svvrhqbwsKCjRhwgTVrVtXvr6+6ty5s3bv3s2+YeA6cMUXMNiuXbvUsWNHVatWTc8995y8vLz02muv6Z577tGnn36qsLAwSb//jz4xMdH+V8nZ2dn69ttvtW3bNnXp0kWS1Lt3b+3atUtPPfWUQkJCdPz4cSUnJ+vw4cOXDTB9+vTRc889pxUrVujZZ591eG7FihW6//77VbNmTeXl5SkyMlK5ubl66qmnFBgYqKNHj+qjjz5SVlaWqlevXqLz/+2333Ty5EmHMTc3N/n5+TmMvf766zpz5oyGDx+u8+fP69///rfuvfde7dy5UwEBAZKkjRs3qmvXrmrYsKEmTJigc+fO6eWXX1b79u21bds2+3vwyy+/KDQ0VFlZWRoyZIiaNGmio0ePatWqVcrJyXG4EvnUU0+pZs2aSkhI0KFDh5SUlKTY2FgtX778iue1YcMG9e7dW82aNVNiYqJOnTpl/0PJ9bhw4YIiIyPVoUMHzZgxQ76+vlec//bbb+vMmTP6xz/+ITc3N02bNk0PP/ywDh48aL9KvGbNGkVFRally5ZKTEzUr7/+qkGDBqlevXrXVesfLV68WOfPn9eQIUNks9lUq1YtZWdna8GCBXr88cc1ePBgnTlzRgsXLlRkZKTS0tLUpk2bqx63T58+atCggRITE7Vt2zYtWLBAderU0dSpU6+69lp6Gx8fr2nTpql79+6KjIzUjh07FBkZqfPnz1/P2wGYzQJwQ1q8eLElyfrmm28uO6dnz56Wt7e39cMPP9jHfvnlF6tq1arW3XffbR9r3bq11a1bt8se59dff7UkWdOnTy92neHh4Vbbtm0dxtLS0ixJ1uuvv25ZlmVt377dkmStXLmy2McvyqX3pqgfm81mn/fjjz9akqxKlSpZP//8s33866+/tiRZI0eOtI+1adPGqlOnjnXq1Cn72I4dOyx3d3drwIAB9rEBAwZY7u7uRfaloKDAob6IiAj7mGVZ1siRIy0PDw8rKyvriufXpk0bKygoyGHehg0bLElW/fr17WObNm2yJFmbNm1yWH/pvBcvXmwfi46OtiRZo0aNKvR60dHRDse9tN7Pz886ffq0ffz999+3JFkffvihfaxly5bWTTfdZJ05c8Y+lpqaWqjWa1G5cmUrOjq6UB3VqlWzjh8/7jD3woULVm5ursPYr7/+agUEBFh///vfHcYlWQkJCfbHCQkJlqRC83r16mX5+fk5jNWvX9+hpmvtbUZGhuXp6Wn17NnT4XgTJkywJDkcE8C1Y6sDYKiLFy9qw4YN6tmzp8NeyaCgIPXt21ebN29Wdna2JKlGjRratWuX/vOf/xR5rEqVKsnb21upqan2rQnXKioqSlu3btUPP/xgH1u+fLlsNpt69OghSfYruuvXry/0V8nXY/bs2UpOTnb4+fjjjwvN69mzp8MVyNDQUIWFhWnt2rWSpGPHjik9PV0DBw5UrVq17PNatWqlLl262OcVFBRo9erV6t69e5F7i93c3BweDxkyxGGsY8eOunjxYpFbQy65VEt0dLTDlfAuXbqoWbNmV3tLrurJJ5+85rlRUVGqWbOm/XHHjh0lSQcPHpT0+9XvnTt3asCAAQ4fQuvUqZNatmx53bVe0rt3b9WuXdthzMPDw351vaCgQKdPn9aFCxfUrl07bdu27ZqOO3ToUIfHHTt21KlTp+y/N1dytd6mpKTowoULGjZsmMO6p5566ppqA1A0gi9gqBMnTignJ0e33XZboeeaNm2qgoICHTlyRNLvdz/IyspS48aN1bJlSz377LP67rvv7PNtNpumTp2qjz/+WAEBAbr77rs1bdo0ZWRkXLWORx99VO7u7va/4rUsSytXrrTvO5akBg0aKC4uTgsWLJC/v78iIyM1e/bs697fGxoaqoiICIefzp07F5p36623Fhpr3Lixff/ypbByuffy5MmTOnv2rE6cOKHs7Gy1aNHimuq7+eabHR5fCpFX+sPFpVqKqrmo+orD09OzWNslrlb/pVqLuouGM++sUdSdOyRp6dKlatWqlX3feu3atbVmzZpr/veqJP251rWXe29q1arl8IcJAMVD8AVwVXfffbd++OEHLVq0SC1atNCCBQt0xx13aMGCBfY5Tz/9tPbv36/ExET5+Pho3Lhxatq0qbZv337FY9etW1cdO3bUihUrJElfffWVDh8+rKioKId5M2fO1HfffafRo0fr3Llz+uc//6nmzZvr559/dv4JlxMeHh5FjluW5ZTj//kK8yUXL14sctxmsxXr1lylXf+1qlSpUqGxN998UwMHDlSjRo20cOFCrVu3TsnJybr33nuv+dZs13N+5eW9AUxD8AUMVbt2bfn6+mrfvn2Fntu7d6/c3d0VHBxsH6tVq5ZiYmL0zjvv6MiRI2rVqlWhb7Nq1KiR/ud//kcbNmzQ999/r7y8PM2cOfOqtURFRWnHjh3at2+fli9fLl9fX3Xv3r3QvJYtW2rs2LH67LPP9Pnnn+vo0aOaO3du8U++mIra4rF//377B9bq168vSZd9L/39/VW5cmXVrl1b1apVK/KOEM5yqZaiav5zfZeuHGZlZTmMX2krhTNdqvXAgQOFnitqzJlWrVqlhg0b6t1331X//v0VGRmpiIiIcvPBscu9N6dOnSr2diIA/4/gCxjKw8ND999/v95//32HW45lZmbq7bffVocOHexbDU6dOuWwtkqVKrrllluUm5srScrJySkUGBo1aqSqVava51xJ79695eHhoXfeeUcrV67UX//6V1WuXNn+fHZ2ti5cuOCwpmXLlnJ3d3c4/uHDh7V3795rewOKYfXq1Tp69Kj9cVpamr7++mt17dpV0u/7otu0aaOlS5c6hMjvv/9eGzZs0IMPPihJcnd3V8+ePfXhhx8W+Y16zrja98da/vhX9snJydq9e7fD3Pr168vDw0OfffaZw/irr7563XVci7p166pFixZ6/fXX9d///tc+/umnn2rnzp2l+tqXrrj+8T3/+uuvtWXLllJ93Wt13333ydPTU3PmzHEYf+WVV1xUEXBj4HZmwA1u0aJFWrduXaHxESNG6MUXX1RycrI6dOigYcOGydPTU6+99ppyc3M1bdo0+9xmzZrpnnvuUdu2bVWrVi19++23WrVqlWJjYyX9fvXzvvvuU58+fdSsWTN5enrqvffeU2Zmph577LGr1linTh117txZs2bN0pkzZwptc/jkk08UGxurRx99VI0bN9aFCxf0xhtvyMPDQ71797bPGzBggD799NNrDpAff/xxkUH5rrvucvjA3y233KIOHTroySefVG5urpKSkuTn56fnnnvOPmf69Onq2rWrwsPDNWjQIPvtzKpXr+5wZXzy5MnasGGDOnXqpCFDhqhp06Y6duyYVq5cqc2bN6tGjRrXVPuVJCYmqlu3burQoYP+/ve/6/Tp03r55ZfVvHlzh4BZvXp1Pfroo3r55Zfl5uamRo0a6aOPPtLx48evu4ZrNXnyZPXo0UPt27dXTEyMfv31V73yyitq0aKFQ63O9te//lXvvvuuevXqpW7duunHH3/U3Llz1axZs1J93WsVEBCgESNGaObMmXrooYf0wAMPaMeOHfr444/l7+9/2W0qAK6M4Avc4P58xeiSgQMHqnnz5vr8888VHx+vxMREFRQUKCwsTG+++ab9Hr6S9M9//lMffPCBNmzYoNzcXNWvX18vvvii/d67wcHBevzxx5WSkqI33nhDnp6eatKkiVasWOEQTK8kKipKGzduVNWqVe1XSC9p3bq1IiMj9eGHH+ro0aPy9fVV69at9fHHH+vOO+8s4TsjjR8/vsjxxYsXOwTfAQMGyN3dXUlJSTp+/LhCQ0P1yiuvKCgoyD4nIiJC69atU0JCgsaPHy8vLy916tRJU6dOdfhwVb169fT1119r3Lhxeuutt5Sdna169eqpa9euV70v7rV64IEHtHLlSo0dO1bx8fFq1KiRFi9erPfff7/Ql1W8/PLLys/P19y5c2Wz2dSnTx9Nnz79mj+Ad726d++ud955RxMmTNCoUaN06623asmSJVq6dKl27dpVaq87cOBAZWRk6LXXXtP69evVrFkzvfnmm1q5cmWh98hVpk6dKl9fX82fP18bN25UeHi4NmzYoA4dOlzXt8kBJnOz2EkPAEU6dOiQGjRooOnTp+uZZ55xdTlGadOmjWrXru3w7YD4fT92zZo19eKLL2rMmDGuLgeocNjjCwBwmfz8/EL7t1NTU7Vjxw7dc889rimqnDh37lyhsaSkJEky/r0BSoqtDgAAlzl69KgiIiL0t7/9TXXr1tXevXs1d+5cBQYGFvqCCNMsX75cS5Ys0YMPPqgqVapo8+bNeuedd3T//ferffv2ri4PqJAIvgAAl6lZs6batm2rBQsW6MSJE6pcubK6deumKVOmyM/Pz9XluVSrVq3k6empadOmKTs72/6BtxdffNHVpQEVlkv3+H722WeaPn26tm7dqmPHjum9995Tz549r7gmNTVVcXFx2rVrl4KDgzV27FgNHDiwTOoFAABAxeXSPb5nz55V69atNXv27Gua/+OPP6pbt27q3Lmz0tPT9fTTT+uJJ57Q+vXrS7lSAAAAVHTl5q4Obm5uV73i+/zzz2vNmjUO33r02GOPKSsrq8j7lAIAAACXVKg9vlu2bFFERITDWGRkpJ5++unLrsnNzXX4ZqeCggKdPn1afn5+3AAcAACgHLIsS2fOnFHdunXl7u68DQoVKvhmZGQoICDAYSwgIEDZ2dk6d+6cKlWqVGhNYmKiJk6cWFYlAgAAwEmOHDmim266yWnHq1DBtyTi4+MVFxdnf/zbb7/p5ptv1v79+1WrVi0XVoaykJ+fr02bNqlz587y8vJydTkoZfTbLPTbLPTbLKdPn1bjxo1VtWpVpx63QgXfwMBAZWZmOoxlZmaqWrVqRV7tlSSbzSabzVZovFatWsbfKscE+fn58vX1lZ+fH/+hNAD9Ngv9Ngv9NpOzt6VWqG9uCw8PV0pKisNYcnKywsPDXVQRAAAAKgqXBt///ve/Sk9PV3p6uqTfb1eWnp6uw4cPS/p9m8KAAQPs84cOHaqDBw/queee0969e/Xqq69qxYoVGjlypCvKBwAAQAXi0uD77bff6vbbb9ftt98uSYqLi9Ptt9+u8ePHS5KOHTtmD8GS1KBBA61Zs0bJyclq3bq1Zs6cqQULFigyMtIl9QMAAKDicOke33vuuUdXuo3wkiVLilyzffv2UqwKAAAAN6IKtccXAAAAKCmCLwAAAIxA8AUAAIARCL4AAAAwAsEXAAAARiD4AgAAwAgEXwAAABiB4AsAAAAjEHwBAABgBIIvAAAAjEDwBQAAgBEIvgAAADACwRcAAABGIPgCAADACARfAAAAGIHgCwAAACMQfAEAAGAEgi8AAACMQPAFAACAEQi+AAAAMALBFwAAAEYg+AIAAMAIBF8AAAAYgeALAAAAIxB8AQAAYASCLwAAAIxA8AUAAIARCL4AAAAwAsEXAAAARiD4AgAAwAgEXwAAABiB4AsAAAAjEHwBAABgBIIvAAAAjEDwBQAAgBEIvgAAADACwRcAAABGIPgCAADACARfAAAAGIHgCwAAACMQfAEAAGAEgi8AAACMQPAFAACAEQi+AAAAMALBFwAAAEYg+AIAAMAIBF8AAAAYgeALAAAAIxB8AQAAYASCLwAAAIxA8AUAAIARCL4AAAAwAsEXAAAARiD4AgAAwAgEXwAAABiB4AsAAAAjEHwBAABgBIIvAAAAjEDwBQAAgBEIvgAAADACwRcAAABGIPgCAADACARfAAAAGIHgCwAAACMQfAEAAGAEgi8AAACMQPAFAACAEQi+AAAAMALBFwAAAEYg+AIAAMAIBF8AAAAYgeALAAAAIxB8AQAAYASCLwAAAIxA8AUAAIARCL4AAAAwAsEXAAAARiD4AgAAwAgEXwAAABiB4AsAAAAjEHwBAABgBJcH39mzZyskJEQ+Pj4KCwtTWlraFecnJSXptttuU6VKlRQcHKyRI0fq/PnzZVQtAAAAKiqXBt/ly5crLi5OCQkJ2rZtm1q3bq3IyEgdP368yPlvv/22Ro0apYSEBO3Zs0cLFy7U8uXLNXr06DKuHAAAABWNS4PvrFmzNHjwYMXExKhZs2aaO3eufH19tWjRoiLnf/nll2rfvr369u2rkJAQ3X///Xr88cevepUYAAAA8HTVC+fl5Wnr1q2Kj4+3j7m7uysiIkJbtmwpcs1dd92lN998U2lpaQoNDdXBgwe1du1a9e/f/7Kvk5ubq9zcXPvj7OxsSVJ+fr7y8/OddDYory71mF6bgX6bhX6bhX6bpbT67LLge/LkSV28eFEBAQEO4wEBAdq7d2+Ra/r27auTJ0+qQ4cOsixLFy5c0NChQ6+41SExMVETJ04sNL5p0yb5+vpe30mgwkhOTnZ1CShD9Nss9Nss9NsMOTk5pXJclwXfkkhNTdXkyZP16quvKiwsTAcOHNCIESM0adIkjRs3rsg18fHxiouLsz/Ozs5WcHCwOnfuLD8/v7IqHS6Sn5+v5ORkdenSRV5eXq4uB6WMfpuFfpuFfpvl1KlTpXJclwVff39/eXh4KDMz02E8MzNTgYGBRa4ZN26c+vfvryeeeEKS1LJlS509e1ZDhgzRmDFj5O5eeMuyzWaTzWYrNO7l5cUvjkHot1not1not1notxlKq8cu+3Cbt7e32rZtq5SUFPtYQUGBUlJSFB4eXuSanJycQuHWw8NDkmRZVukVCwAAgArPpVsd4uLiFB0drXbt2ik0NFRJSUk6e/asYmJiJEkDBgxQvXr1lJiYKEnq3r27Zs2apdtvv92+1WHcuHHq3r27PQADAAAARXFp8I2KitKJEyc0fvx4ZWRkqE2bNlq3bp39A2+HDx92uMI7duxYubm5aezYsTp69Khq166t7t2766WXXnLVKQAAAKCCcPmH22JjYxUbG1vkc6mpqQ6PPT09lZCQoISEhDKoDAAAADcSl39lMQAAAFAWCL4AAAAwAsEXAAAARiD4AgAAwAgEXwAAABiB4AsAAAAjEHwBAABgBIIvAAAAjEDwBQAAgBEIvgAAADACwRcAAABGIPgCAADACARfAAAAGIHgCwAAACMQfAEAAGAEgi8AAACMQPAFAACAEQi+AAAAMALBFwAAAEYg+AIAAMAIBF8AAAAYgeALAAAAIxB8AQAAYASCLwAAAIxA8AUAAIARCL4AAAAwAsEXAAAARiD4AgAAwAgEXwAAABiB4AsAAAAjEHwBAABgBIIvAAAAjEDwBQAAgBEIvgAAADACwRcAAABGIPgCAADACARfAAAAGIHgCwAAACMQfAEAAGAEgi8AAACMQPAFAACAEQi+AAAAMALBFwAAAEYg+AIAAMAIBF8AAAAYgeALAAAAIxB8AQAAYASCLwAAAIxA8AUAAIARCL4AAAAwAsEXAAAARiD4AgAAwAgEXwAAABiB4AsAAAAjEHwBAABgBIIvAAAAjEDwBQAAgBEIvgAAADACwRcAAABGIPgCAADACARfAAAAGIHgCwAAACMQfAEAAGAEgi8AAACMQPAFAACAEQi+AAAAMALBFwAAAEYg+AIAAMAIBF8AAAAYgeALAAAAIxB8AQAAYASCLwAAAIxA8AUAAIARCL4AAAAwAsEXAAAARiD4AgAAwAgEXwAAABiB4AsAAAAjEHwBAABgBIIvAAAAjEDwBQAAgBEIvgAAADCCy4Pv7NmzFRISIh8fH4WFhSktLe2K87OysjR8+HAFBQXJZrOpcePGWrt2bRlVCwAAgIrK05Uvvnz5csXFxWnu3LkKCwtTUlKSIiMjtW/fPtWpU6fQ/Ly8PHXp0kV16tTRqlWrVK9ePf3000+qUaNG2RcPAACACsWlwXfWrFkaPHiwYmJiJElz587VmjVrtGjRIo0aNarQ/EWLFun06dP68ssv5eXlJUkKCQkpy5IBAABQQbks+Obl5Wnr1q2Kj4+3j7m7uysiIkJbtmwpcs0HH3yg8PBwDR8+XO+//75q166tvn376vnnn5eHh0eRa3Jzc5Wbm2t/nJ2dLUnKz89Xfn6+E88I5dGlHtNrM9Bvs9Bvs9Bvs5RWn10WfE+ePKmLFy8qICDAYTwgIEB79+4tcs3Bgwf1ySefqF+/flq7dq0OHDigYcOGKT8/XwkJCUWuSUxM1MSJEwuNb9q0Sb6+vtd/IqgQkpOTXV0CyhD9Ngv9Ngv9NkNOTk6pHNelWx2Kq6CgQHXq1NG8efPk4eGhtm3b6ujRo5o+ffplg298fLzi4uLsj7OzsxUcHKzOnTvLz8+vrEqHi+Tn5ys5OVldunSxb4/BjYt+m4V+m4V+m+XUqVOlclyXBV9/f395eHgoMzPTYTwzM1OBgYFFrgkKCpKXl5fDtoamTZsqIyNDeXl58vb2LrTGZrPJZrMVGvfy8uIXxyD02yz02yz02yz02wyl1WOX3c7M29tbbdu2VUpKin2soKBAKSkpCg8PL3JN+/btdeDAARUUFNjH9u/fr6CgoCJDLwAAAHCJS+/jGxcXp/nz52vp0qXas2ePnnzySZ09e9Z+l4cBAwY4fPjtySef1OnTpzVixAjt379fa9as0eTJkzV8+HBXnQIAAAAqCJfu8Y2KitKJEyc0fvx4ZWRkqE2bNlq3bp39A2+HDx+Wu/v/Z/Pg4GCtX79eI0eOVKtWrVSvXj2NGDFCzz//vKtOAQAAABWEyz/cFhsbq9jY2CKfS01NLTQWHh6ur776qpSrAgAAwI3G5V9ZDAAAAJQFgi8AAACMQPAFAACAEQi+AAAAMALBFwAAAEYg+AIAAMAIBF8AAAAYgeALAAAAIxB8AQAAYASCLwAAAIxA8AUAAIARCL4AAAAwAsEXAAAARiD4AgAAwAgEXwAAABiB4AsAAAAjEHwBAABgBIIvAAAAjEDwBQAAgBEIvgAAADACwRcAAABGIPgCAADACARfAAAAGIHgCwAAACMQfAEAAGAEgi8AAACMQPAFAACAEUoUfI8cOaKff/7Z/jgtLU1PP/205s2b57TCAAAAAGcqUfDt27evNm3aJEnKyMhQly5dlJaWpjFjxuiFF15waoEAAACAM5Qo+H7//fcKDQ2VJK1YsUItWrTQl19+qbfeektLlixxZn0AAACAU5Qo+Obn58tms0mSNm7cqIceekiS1KRJEx07dsx51QEAAABOUqLg27x5c82dO1eff/65kpOT9cADD0iSfvnlF/n5+Tm1QAAAAMAZShR8p06dqtdee0333HOPHn/8cbVu3VqS9MEHH9i3QAAAAADliWdJFt1zzz06efKksrOzVbNmTfv4kCFD5Ovr67TiAAAAAGcp0RXfc+fOKTc31x56f/rpJyUlJWnfvn2qU6eOUwsEAAAAnKFEwbdHjx56/fXXJUlZWVkKCwvTzJkz1bNnT82ZM8epBQIAAADOUKLgu23bNnXs2FGStGrVKgUEBOinn37S66+/rv/93/91aoEAAACAM5Qo+Obk5Khq1aqSpA0bNujhhx+Wu7u77rzzTv30009OLRAAAABwhhIF31tuuUWrV6/WkSNHtH79et1///2SpOPHj6tatWpOLRAAAABwhhIF3/Hjx+uZZ55RSEiIQkNDFR4eLun3q7+33367UwsEAAAAnKFEtzN75JFH1KFDBx07dsx+D19Juu+++9SrVy+nFQcAAAA4S4mCryQFBgYqMDBQP//8syTppptu4ssrAAAAUG6VaKtDQUGBXnjhBVWvXl3169dX/fr1VaNGDU2aNEkFBQXOrhEAAAC4biW64jtmzBgtXLhQU6ZMUfv27SVJmzdv1oQJE3T+/Hm99NJLTi0SAAAAuF4lCr5Lly7VggUL9NBDD9nHWrVqpXr16mnYsGEEXwAAAJQ7JdrqcPr0aTVp0qTQeJMmTXT69OnrLgoAAABwthIF39atW+uVV14pNP7KK6+oVatW110UAAAA4Gwl2uowbdo0devWTRs3brTfw3fLli06cuSI1q5d69QCAQAAAGco0RXfTp06af/+/erVq5eysrKUlZWlhx9+WLt27dIbb7zh7BoBAACA61bi+/jWrVu30IfYduzYoYULF2revHnXXRgAAADgTCW64gsAAABUNARfAAAAGIHgCwAAACMUa4/vww8/fMXns7KyrqcWAAAAoNQUK/hWr179qs8PGDDgugoCAAAASkOxgu/ixYtLqw4AAACgVLHHFwAAAEYg+AIAAMAIBF8AAAAYgeALAAAAIxB8AQAAYASCLwAAAIxA8AUAAIARCL4AAAAwAsEXAAAARiD4AgAAwAgEXwAAABiB4AsAAAAjEHwBAABgBIIvAAAAjEDwBQAAgBEIvgAAADACwRcAAABGIPgCAADACARfAAAAGIHgCwAAACMQfAEAAGAEgi8AAACMQPAFAACAEQi+AAAAMALBFwAAAEYoF8F39uzZCgkJkY+Pj8LCwpSWlnZN65YtWyY3Nzf17NmzdAsEAABAhefy4Lt8+XLFxcUpISFB27ZtU+vWrRUZGanjx49fcd2hQ4f0zDPPqGPHjmVUKQAAACoylwffWbNmafDgwYqJiVGzZs00d+5c+fr6atGiRZddc/HiRfXr108TJ05Uw4YNy7BaAAAAVFSernzxvLw8bd26VfHx8fYxd3d3RUREaMuWLZdd98ILL6hOnToaNGiQPv/88yu+Rm5urnJzc+2Ps7OzJUn5+fnKz8+/zjNAeXepx/TaDPTbLPTbLPTbLKXVZ5cG35MnT+rixYsKCAhwGA8ICNDevXuLXLN582YtXLhQ6enp1/QaiYmJmjhxYqHxTZs2ydfXt9g1o2JKTk52dQkoQ/TbLPTbLPTbDDk5OaVyXJcG3+I6c+aM+vfvr/nz58vf3/+a1sTHxysuLs7+ODs7W8HBwercubP8/PxKq1SUE/n5+UpOTlaXLl3k5eXl6nJQyui3Wei3Wei3WU6dOlUqx3Vp8PX395eHh4cyMzMdxjMzMxUYGFho/g8//KBDhw6pe/fu9rGCggJJkqenp/bt26dGjRo5rLHZbLLZbIWO5eXlxS+OQei3Wei3Wei3Wei3GUqrxy79cJu3t7fatm2rlJQU+1hBQYFSUlIUHh5eaH6TJk20c+dOpaen238eeughde7cWenp6QoODi7L8gEAAFCBuHyrQ1xcnKKjo9WuXTuFhoYqKSlJZ8+eVUxMjCRpwIABqlevnhITE+Xj46MWLVo4rK9Ro4YkFRoHAAAA/sjlwTcqKkonTpzQ+PHjlZGRoTZt2mjdunX2D7wdPnxY7u4uv+saAAAAKjiXB19Jio2NVWxsbJHPpaamXnHtkiVLnF8QAAAAbjhcSgUAAIARCL4AAAAwAsEXAAAARiD4AgAAwAgEXwAAABiB4AsAAAAjEHwBAABgBIIvAAAAjEDwBQAAgBEIvgAAADACwRcAAABGIPgCAADACARfAAAAGIHgCwAAACMQfAEAAGAEgi8AAACMQPAFAACAEQi+AAAAMALBFwAAAEYg+AIAAMAIBF8AAAAYgeALAAAAIxB8AQAAYASCLwAAAIxA8AUAAIARCL4AAAAwAsEXAAAARiD4AgAAwAgEXwAAABiB4AsAAAAjEHwBAABgBIIvAAAAjEDwBQAAgBEIvgAAADACwRcAAABGIPgCAADACARfAAAAGIHgCwAAACMQfAEAAGAEgi8AAACMQPAFAACAEQi+AAAAMALBFwAAAEYg+AIAAMAIBF8AAAAYgeALAAAAIxB8AQAAYASCLwAAAIxA8AUAAIARCL4AAAAwAsEXAAAARiD4AgAAwAgEXwAAABiB4AsAAAAjEHwBAABgBIIvAAAAjEDwBQAAgBEIvgAAADACwRcAAABGIPgCAADACARfAAAAGIHgCwAAACMQfAEAAGAEgi8AAACMQPAFAACAEQi+AAAAMALBFwAAAEYg+AIAAMAIBF8AAAAYgeALAAAAIxB8AQAAYASCLwAAAIxA8AUAAIARCL4AAAAwAsEXAAAARiD4AgAAwAgEXwAAABiB4AsAAAAjEHwBAABgBIIvAAAAjEDwBQAAgBHKRfCdPXu2QkJC5OPjo7CwMKWlpV127vz589WxY0fVrFlTNWvWVERExBXnAwAAAFI5CL7Lly9XXFycEhIStG3bNrVu3VqRkZE6fvx4kfNTU1P1+OOPa9OmTdqyZYuCg4N1//336+jRo2VcOQAAACoSlwffWbNmafDgwYqJiVGzZs00d+5c+fr6atGiRUXOf+uttzRs2DC1adNGTZo00YIFC1RQUKCUlJQyrhwAAAAViacrXzwvL09bt25VfHy8fczd3V0RERHasmXLNR0jJydH+fn5qlWrVpHP5+bmKjc31/44OztbkpSfn6/8/PzrqB4VwaUe02sz0G+z0G+z0G+zlFafXRp8T548qYsXLyogIMBhPCAgQHv37r2mYzz//POqW7euIiIiinw+MTFREydOLDS+adMm+fr6Fr9oVEjJycmuLgFliH6bhX6bhX6bIScnp1SO69Lge72mTJmiZcuWKTU1VT4+PkXOiY+PV1xcnP1xdna2goOD1blzZ/n5+ZVVqXCR/Px8JScnq0uXLvLy8nJ1OShl9Nss9Nss9Nssp06dKpXjujT4+vv7y8PDQ5mZmQ7jmZmZCgwMvOLaGTNmaMqUKdq4caNatWp12Xk2m002m63QuJeXF784BqHfZqHfZqHfZqHfZiitHrv0w23e3t5q27atwwfTLn1QLTw8/LLrpk2bpkmTJmndunVq165dWZQKAACACs7lWx3i4uIUHR2tdu3aKTQ0VElJSTp79qxiYmIkSQMGDFC9evWUmJgoSZo6darGjx+vt99+WyEhIcrIyJAkValSRVWqVHHZeQAAAKB8c3nwjYqK0okTJzR+/HhlZGSoTZs2Wrdunf0Db4cPH5a7+/9fmJ4zZ47y8vL0yCOPOBwnISFBEyZMKMvSAQAAUIG4PPhKUmxsrGJjY4t8LjU11eHxoUOHSr8gAAAA3HBc/gUWAAAAQFkg+AIAAMAIBF8AAAAYgeALAAAAIxB8AQAAYASCLwAAAIxA8AUAAIARCL4AAAAwAsEXAAAARiD4AgAAwAgEXwAAABiB4AsAAAAjEHwBAABgBIIvAAAAjEDwBQAAgBEIvgAAADACwRcAAABGIPgCAADACARfAAAAGIHgCwAAACMQfAEAAGAEgi8AAACMQPAFAACAEQi+AAAAMALBFwAAAEYg+AIAAMAIBF8AAAAYgeALAAAAIxB8AQAAYASCLwAAAIxA8AUAAIARCL4AAAAwAsEXAAAARiD4AgAAwAgEXwAAABiB4AsAAAAjEHwBAABgBIIvAAAAjEDwBQAAgBEIvgAAADACwRcAAABGIPgCAADACARfAAAAGIHgCwAAACMQfAEAAGAEgi8AAACMQPAFAACAEQi+AAAAMALBFwAAAEYg+AIAAMAIBF8AAAAYgeALAAAAIxB8AQAAYASCLwAAAIxA8AUAAIARCL4AAAAwAsEXAAAARiD4AgAAwAgEXwAAABiB4AsAAAAjEHwBAABgBIIvAAAAjEDwBQAAgBEIvgAAADACwRcAAABGIPgCAADACARfAAAAGIHgCwAAACMQfAEAAGAEgi8AAACMQPAFAACAEQi+AAAAMALBFwAAAEYg+AIAAMAIBF8AAAAYgeALAAAAIxB8AQAAYASCLwAAAIxA8AUAAIARCL4AAAAwAsEXAAAARiD4AgAAwAjlIvjOnj1bISEh8vHxUVhYmNLS0q44f+XKlWrSpIl8fHzUsmVLrV27towqBQAAQEXl8uC7fPlyxcXFKSEhQdu2bVPr1q0VGRmp48ePFzn/yy+/1OOPP65BgwZp+/bt6tmzp3r27Knvv/++jCsHAABAReLy4Dtr1iwNHjxYMTExatasmebOnStfX18tWrSoyPn//ve/9cADD+jZZ59V06ZNNWnSJN1xxx165ZVXyrhyAAAAVCSernzxvLw8bd26VfHx8fYxd3d3RUREaMuWLUWu2bJli+Li4hzGIiMjtXr16iLn5+bmKjc31/74t99+kySdPn36OqtHRZCfn6+cnBydOnVKXl5eri4HpYx+m4V+m4V+m+VSTrMsy6nHdWnwPXnypC5evKiAgACH8YCAAO3du7fINRkZGUXOz8jIKHJ+YmKiJk6cWGi8cePGJawaAAAAZeHUqVOqXr26047n0uBbFuLj4x2uEGdlZal+/fo6fPiwU99IlE/Z2dkKDg7WkSNHVK1aNVeXg1JGv81Cv81Cv83y22+/6eabb1atWrWcelyXBl9/f395eHgoMzPTYTwzM1OBgYFFrgkMDCzWfJvNJpvNVmi8evXq/OIYpFq1avTbIPTbLPTbLPTbLO7uzv04mks/3Obt7a22bdsqJSXFPlZQUKCUlBSFh4cXuSY8PNxhviQlJydfdj4AAAAglYOtDnFxcYqOjla7du0UGhqqpKQknT17VjExMZKkAQMGqF69ekpMTJQkjRgxQp06ddLMmTPVrVs3LVu2TN9++63mzZvnytMAAABAOefy4BsVFaUTJ05o/PjxysjIUJs2bbRu3Tr7B9gOHz7scJn7rrvu0ttvv62xY8dq9OjRuvXWW7V69Wq1aNHiml7PZrMpISGhyO0PuPHQb7PQb7PQb7PQb7OUVr/dLGffJwIAAAAoh1z+BRYAAABAWSD4AgAAwAgEXwAAABiB4AsAAAAj3JDBd/bs2QoJCZGPj4/CwsKUlpZ2xfkrV65UkyZN5OPjo5YtW2rt2rVlVCmcoTj9nj9/vjp27KiaNWuqZs2aioiIuOq/Hyhfivv7fcmyZcvk5uamnj17lm6BcKri9jsrK0vDhw9XUFCQbDabGjduzH/TK5Di9jspKUm33XabKlWqpODgYI0cOVLnz58vo2pxPT777DN1795ddevWlZubm1avXn3VNampqbrjjjtks9l0yy23aMmSJcV/YesGs2zZMsvb29tatGiRtWvXLmvw4MFWjRo1rMzMzCLnf/HFF5aHh4c1bdo0a/fu3dbYsWMtLy8va+fOnWVcOUqiuP3u27evNXv2bGv79u3Wnj17rIEDB1rVq1e3fv755zKuHCVR3H5f8uOPP1r16tWzOnbsaPXo0aNsisV1K26/c3NzrXbt2lkPPvigtXnzZuvHH3+0UlNTrfT09DKuHCVR3H6/9dZbls1ms9566y3rxx9/tNavX28FBQVZI0eOLOPKURJr1661xowZY7377ruWJOu999674vyDBw9avr6+VlxcnLV7927r5Zdftjw8PKx169YV63VvuOAbGhpqDR8+3P744sWLVt26da3ExMQi5/fp08fq1q2bw1hYWJj1j3/8o1TrhHMUt99/duHCBatq1arW0qVLS6tEOFFJ+n3hwgXrrrvushYsWGBFR0cTfCuQ4vZ7zpw5VsOGDa28vLyyKhFOVNx+Dx8+3Lr33nsdxuLi4qz27duXap1wvmsJvs8995zVvHlzh7GoqCgrMjKyWK91Q211yMvL09atWxUREWEfc3d3V0REhLZs2VLkmi1btjjMl6TIyMjLzkf5UZJ+/1lOTo7y8/NVq1at0ioTTlLSfr/wwguqU6eOBg0aVBZlwklK0u8PPvhA4eHhGj58uAICAtSiRQtNnjxZFy9eLKuyUUIl6fddd92lrVu32rdDHDx4UGvXrtWDDz5YJjWjbDkrr7n8m9uc6eTJk7p48aL9W98uCQgI0N69e4tck5GRUeT8jIyMUqsTzlGSfv/Z888/r7p16xb6ZUL5U5J+b968WQsXLlR6enoZVAhnKkm/Dx48qE8++UT9+vXT2rVrdeDAAQ0bNkz5+flKSEgoi7JRQiXpd9++fXXy5El16NBBlmXpwoULGjp0qEaPHl0WJaOMXS6vZWdn69y5c6pUqdI1HeeGuuILFMeUKVO0bNkyvffee/Lx8XF1OXCyM2fOqH///po/f778/f1dXQ7KQEFBgerUqaN58+apbdu2ioqK0pgxYzR37lxXl4ZSkJqaqsmTJ+vVV1/Vtm3b9O6772rNmjWaNGmSq0tDOXZDXfH19/eXh4eHMjMzHcYzMzMVGBhY5JrAwMBizUf5UZJ+XzJjxgxNmTJFGzduVKtWrUqzTDhJcfv9ww8/6NChQ+revbt9rKCgQJLk6empffv2qVGjRqVbNEqsJL/fQUFB8vLykoeHh32sadOmysjIUF5enry9vUu1ZpRcSfo9btw49e/fX0888YQkqWXLljp79qyGDBmiMWPGyN2da3s3ksvltWrVql3z1V7pBrvi6+3trbZt2yolJcU+VlBQoJSUFIWHhxe5Jjw83GG+JCUnJ192PsqPkvRbkqZNm6ZJkyZp3bp1ateuXVmUCicobr+bNGminTt3Kj093f7z0EMPqXPnzkpPT1dwcHBZlo9iKsnvd/v27XXgwAH7H3Akaf/+/QoKCiL0lnMl6XdOTk6hcHvpDz2/f14KNxKn5bXife6u/Fu2bJlls9msJUuWWLt377aGDBli1ahRw8rIyLAsy7L69+9vjRo1yj7/iy++sDw9Pa0ZM2ZYe/bssRISEridWQVS3H5PmTLF8vb2tlatWmUdO3bM/nPmzBlXnQKKobj9/jPu6lCxFLffhw8ftqpWrWrFxsZa+/btsz766COrTp061osvvuiqU0AxFLffCQkJVtWqVa133nnHOnjwoLVhwwarUaNGVp8+fVx1CiiGM2fOWNu3b7e2b99uSbJmzZplbd++3frpp58sy7KsUaNGWf3797fPv3Q7s2effdbas2ePNXv2bG5ndsnLL79s3XzzzZa3t7cVGhpqffXVV/bnOnXqZEVHRzvMX7FihdW4cWPL29vbat68ubVmzZoyrhjXozj9rl+/viWp0E9CQkLZF44SKe7v9x8RfCue4vb7yy+/tMLCwiybzWY1bNjQeumll6wLFy6UcdUoqeL0Oz8/35owYYLVqFEjy8fHxwoODraGDRtm/frrr2VfOIpt06ZNRf7/+FKPo6OjrU6dOhVa06ZNG8vb29tq2LChtXjx4mK/rptl8fcBAAAAuPHdUHt8AQAAgMsh+AIAAMAIBF8AAAAYgeALAAAAIxB8AQAAYASCLwAAAIxA8AUAAIARCL4AAAAwAsEXAAzl5uam1atXu7oMACgzBF8AcIGBAwfKzc2t0M8DDzzg6tIA4Ibl6eoCAMBUDzzwgBYvXuwwZrPZXFQNANz4uOILAC5is9kUGBjo8FOzZk1Jv29DmDNnjrp27apKlSqpYcOGWrVqlcP6nTt36t5771WlSpXk5+enIUOG6L///a/DnEWLFql58+ay2WwKCgpSbGysw/MnT55Ur1695Ovrq1tvvVUffPBB6Z40ALgQwRcAyqlx48apd+/e2rFjh/r166fHHntMe/bskSSdPXtWkZGRqlmzpr755hutXLlSGzdudAi2c+bM0fDhwzVkyBDt3LlTH3zwgW655RaH15g4caL69Omj7777Tg8++KD69eun06dPl+l5AkBZcbMsy3J1EQBgmoEDB+rNN9+Uj4+Pw/jo0aM1evRoubm5aejQoZozZ479uTvvvFN33HGHXn31Vc2fP1/PP/+8jhw5osqVK0uS1q5dq+7du+uXX35RQECA6tWrp5iYGL344otF1uDm5qaxY8dq0qRJkn4P01WqVNHHH3/MXmMANyT2+AKAi3Tu3Nkh2EpSrVq17P8cHh7u8Fx4eLjS09MlSXv27FHr1q3toVeS2rdvr4KCAu3bt09ubm765ZdfdN99912xhlatWtn/uXLlyqpWrZqOHz9e0lMCgHKN4AsALlK5cuVCWw+cpVKlStc0z8vLy+Gxm5ubCgoKSqMkAHA59vgCQDn11VdfFXrctGlTSVLTpk21Y8cOnT171v78F198IXd3d912222qWrWqQkJClJKSUqY1A0B5xhVfAHCR3NxcZWRkOIx5enrK399fkrRy5Uq1a9dOHTp00FtvvaW0tDQtXLhQktSvXz8lJCQoOjpaEyZM0IkTJ/TUU0+pf//+CggIkCRNmDBBQ4cOVZ06ddS1a1edOXNGX3zxhZ566qmyPVEAKCcIvgDgIuvWrVNQUJDD2G233aa9e/dK+v2OC8uWLdOwYcMUFBSkd955R82aNZMk+fr6av369RoxYoT+8pe/yNfXV71799asWbPsx4qOjtb58+f1r3/9S88884z8/f31yCOPlN0JAkA5w10dAKAccnNz03vvvaeePXu6uhQAuGGwxxcAAABGIPgCAADACOzxBYByiF1oAOB8XPEFAACAEQi+AAAAMALBFwAAAEYg+AIAAMAIBF8AAAAYgeALAAAAIxB8AQAAYASCLwAAAIzwf5SYtZANdJ/iAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result_path = createResultFolder(osp.join(\"./results\", model_name))\n",
    "test_best_loss = None\n",
    "epoch = None\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "val_losses = []\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss vs. Epoch during Training\")\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.3503, 0.0000,  ..., 0.1708, 0.0000, 0.4166],\n",
      "        [0.0000, 0.3503, 0.0000,  ..., 0.1708, 0.0000, 0.4166],\n",
      "        [0.0000, 0.3503, 0.0000,  ..., 0.1708, 0.0000, 0.4166],\n",
      "        ...,\n",
      "        [1.4153, 0.0000, 0.0000,  ..., 1.8420, 0.0000, 0.6478],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.2514],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.2514]],\n",
      "       device='cuda:0', grad_fn=<ReluBackward0>) tensor([[    0,     0,     0,  ..., 27980, 27981, 27982],\n",
      "        [    2,     4,     5,  ..., 27980, 27981, 27982]], device='cuda:0') tensor([   0,    0,    0,  ..., 1023, 1023, 1023], device='cuda:0') tensor([[3.7711],\n",
      "        [3.7535],\n",
      "        [4.0193],\n",
      "        ...,\n",
      "        [0.0000],\n",
      "        [0.0000],\n",
      "        [0.0000]], device='cuda:0')\n",
      "tensor([[[0.0000, 0.3503, 0.0000,  ..., 0.1708, 0.0000, 0.4166]],\n",
      "\n",
      "        [[0.0000, 0.3503, 0.0000,  ..., 0.1708, 0.0000, 0.4166]],\n",
      "\n",
      "        [[0.0000, 0.3503, 0.0000,  ..., 0.1708, 0.0000, 0.4166]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1.4153, 0.0000, 0.0000,  ..., 1.8420, 0.0000, 0.6478]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.2514]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.2514]]],\n",
      "       device='cuda:0', grad_fn=<RepeatBackward0>)\n",
      "message [tensor([[ 0.9170, -0.0943,  0.5727,  ..., -0.4115,  0.0991,  0.4394],\n",
      "        [ 0.9152, -0.0971,  0.5695,  ..., -0.4127,  0.1003,  0.4390],\n",
      "        [ 0.9421, -0.0556,  0.6180,  ..., -0.3955,  0.0825,  0.4456],\n",
      "        ...,\n",
      "        [ 0.7368, -0.6915, -0.3361,  ...,  0.0372,  0.1304, -0.0995],\n",
      "        [ 0.2856, -0.7171,  0.2783,  ..., -0.4058,  0.3250, -0.0350],\n",
      "        [ 0.2856, -0.7171,  0.2783,  ..., -0.4058,  0.3250, -0.0350]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)]\n",
      "aggregate [tensor([[[-4.2515e+01, -3.7696e+00,  8.4411e+00,  ...,  3.4281e-02,\n",
      "           4.9587e-02,  1.4680e-02]],\n",
      "\n",
      "        [[-4.2515e+01, -3.7696e+00,  8.4411e+00,  ...,  3.4281e-02,\n",
      "           4.9587e-02,  1.4680e-02]],\n",
      "\n",
      "        [[-4.2515e+01, -3.7696e+00,  8.4411e+00,  ...,  3.4281e-02,\n",
      "           4.9587e-02,  1.4680e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-2.7365e+01,  3.1503e+00,  2.6892e+00,  ...,  1.8639e-02,\n",
      "           2.4240e-02,  2.0336e-02]],\n",
      "\n",
      "        [[-1.0717e+01,  7.6122e+00,  9.1028e+00,  ...,  3.3244e-02,\n",
      "           2.0788e-02,  1.1735e-02]],\n",
      "\n",
      "        [[-1.0717e+01,  7.6122e+00,  9.1028e+00,  ...,  3.3244e-02,\n",
      "           2.0788e-02,  1.1735e-02]]], device='cuda:0', grad_fn=<CatBackward0>)]\n",
      "tensor([[[-4.2515e+01, -3.7696e+00,  8.4411e+00,  ...,  3.4281e-02,\n",
      "           4.9587e-02,  1.4680e-02]],\n",
      "\n",
      "        [[-4.2515e+01, -3.7696e+00,  8.4411e+00,  ...,  3.4281e-02,\n",
      "           4.9587e-02,  1.4680e-02]],\n",
      "\n",
      "        [[-4.2515e+01, -3.7696e+00,  8.4411e+00,  ...,  3.4281e-02,\n",
      "           4.9587e-02,  1.4680e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-2.7365e+01,  3.1503e+00,  2.6892e+00,  ...,  1.8639e-02,\n",
      "           2.4240e-02,  2.0336e-02]],\n",
      "\n",
      "        [[-1.0717e+01,  7.6122e+00,  9.1028e+00,  ...,  3.3244e-02,\n",
      "           2.0788e-02,  1.1735e-02]],\n",
      "\n",
      "        [[-1.0717e+01,  7.6122e+00,  9.1028e+00,  ...,  3.3244e-02,\n",
      "           2.0788e-02,  1.1735e-02]]], device='cuda:0', grad_fn=<CatBackward0>)\n",
      "tensor([[  7.2633,  -3.3210,  -0.9883,  ...,  12.5504,  -6.9890, -29.1864],\n",
      "        [  7.2633,  -3.3210,  -0.9883,  ...,  12.5504,  -6.9890, -29.1865],\n",
      "        [  7.2633,  -3.3210,  -0.9883,  ...,  12.5503,  -6.9890, -29.1865],\n",
      "        ...,\n",
      "        [ -1.4865,  -5.5041,   1.2786,  ...,   0.3136,  -1.9291,  -8.1412],\n",
      "        [  2.5485,  -3.8381,   1.2869,  ...,   2.5068,  -3.6214,  -8.2982],\n",
      "        [  2.5485,  -3.8381,   1.2869,  ...,   2.5068,  -3.6214,  -8.2982]],\n",
      "       device='cuda:0', grad_fn=<CatBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument weight in method wrapper_CUDA__native_batch_norm)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m epochs \u001b[38;5;241m=\u001b[39m model_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epoch_start \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m----> 7\u001b[0m     model, train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     val_loss, _, _ \u001b[38;5;241m=\u001b[39m test_evaluations(model, val_loader, validation_dataset, device, ret_data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m     test_loss, _, _ \u001b[38;5;241m=\u001b[39m test_evaluations(model, test_loader, test_dataset, device, ret_data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/data/Material/train.py:32\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(model, train_data_loader, train_dataset, optimizer, device)\u001b[0m\n\u001b[1;32m     30\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     31\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 32\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch_size, formation_energy_per_atom]\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# use Mean Absolute Error to calculate loss\u001b[39;00m\n\u001b[1;32m     34\u001b[0m loss \u001b[38;5;241m=\u001b[39m (out\u001b[38;5;241m.\u001b[39msqueeze() \u001b[38;5;241m-\u001b[39m data\u001b[38;5;241m.\u001b[39my)\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[0;32m~/anaconda3/envs/material/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/material/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/data/Material/model.py:108\u001b[0m, in \u001b[0;36mCEALNetwork.forward\u001b[0;34m(self, batch_data)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m conv, bn \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbns):\n\u001b[1;32m    107\u001b[0m     out \u001b[38;5;241m=\u001b[39m conv(out, edge_index, batch, edge_attr)\n\u001b[0;32m--> 108\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mbn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m     out \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(out)\n\u001b[1;32m    110\u001b[0m     out \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(out, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_rate, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n",
      "File \u001b[0;32m~/anaconda3/envs/material/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/material/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/material/lib/python3.12/site-packages/torch/nn/modules/batchnorm.py:175\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    168\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    170\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/material/lib/python3.12/site-packages/torch/nn/functional.py:2509\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m   2507\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m-> 2509\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2510\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[1;32m   2511\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument weight in method wrapper_CUDA__native_batch_norm)"
     ]
    }
   ],
   "source": [
    "epoch_start = 0 if epoch is None else epoch\n",
    "epoch = epoch_start\n",
    "epochs = model_args[\"epochs\"]\n",
    "\n",
    "for epoch in range(epoch_start + 1, epochs + 1):\n",
    "\n",
    "    model, train_loss = train_step(model, train_loader, train_dataset, optimizer, device)\n",
    "    val_loss, _, _ = test_evaluations(model, val_loader, validation_dataset, device, ret_data=False)\n",
    "    test_loss, _, _ = test_evaluations(model, test_loader, test_dataset, device, ret_data=False)\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "    current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    test_losses.append(test_loss)\n",
    "\n",
    "    plot_training_progress(len(train_losses), train_losses, val_losses, test_losses, split=15)\n",
    "\n",
    "    # save best model\n",
    "    if test_best_loss is None or test_loss < test_best_loss:\n",
    "        test_best_loss = test_loss\n",
    "        save_model_GCN(epoch, model, optimizer, scheduler, result_path)\n",
    "\n",
    "    progress_msg = \"Epoch \" + str(epoch)\n",
    "    progress_msg += \", train loss(MAE)=\" + str(round(train_loss, 4))\n",
    "    progress_msg += \", valid loss(MAE)=\" + str(round(val_loss, 4))\n",
    "    progress_msg += \", test loss(MAE)=\" + str(round(test_loss, 4))\n",
    "    progress_msg += \", lr=\" + str(round(current_lr, 8))\n",
    "    progress_msg += \", best_test=\" + str(round(test_best_loss, 4))\n",
    "\n",
    "    print(progress_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model and checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(osp.join(result_path, \"checkpoint.pt\"),\n",
    "                        map_location=get_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = checkpoint[\"model\"]\n",
    "\n",
    "save_hyper_parameter(args, result_path)\n",
    "save_train_progress(epoch - 1, train_losses, val_losses, test_losses, result_path)\n",
    "test_loss, test_out, test_y = test_evaluations(best_model, test_loader, test_dataset, device, ret_data=True)\n",
    "\n",
    "# Reverse normalization of test_out and y\n",
    "min, max = get_data_scale(args)\n",
    "test_y = reverse_min_max_scalar_1d(test_y, min, max)\n",
    "test_out = reverse_min_max_scalar_1d(test_out, min, max)\n",
    "loss = (test_out.squeeze() - test_y).abs().mean()\n",
    "print(\"MAE loss of formation energy is: \", loss.item())\n",
    "\n",
    "# save results\n",
    "plot_training_progress(len(train_losses), train_losses, val_losses, test_losses, res_path=result_path)\n",
    "save_regression_result(test_out, test_y, result_path)\n",
    "plot_regression_result(\"GCN\", result_path, plotfilename=\"regression_figure.jpeg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "683727\n",
      "(0, 8)\n",
      "(1, 3)\n",
      "(2, 9)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9365996964735980541"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from dataset import *\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "import numpy as np\n",
    "\n",
    "seed = np.random.randint(1, 1e6)\n",
    "print(seed)\n",
    "\n",
    "generator1 = torch.Generator().manual_seed(40)\n",
    "generator2 = torch.Generator().manual_seed(40)\n",
    "a = random_split(range(10), [3, 7], generator=generator1)\n",
    "b = random_split(range(30), [0.3, 0.3, 0.4], generator=generator2)\n",
    "\n",
    "x = a[0]\n",
    "y = a[1]\n",
    "\n",
    "for i in enumerate(x):\n",
    "    print(i)\n",
    "\n",
    "torch.initial_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_device()\n",
    "\n",
    "args = {}\n",
    "args[\"max_cutoff_distance\"] = 4.0\n",
    "args[\"raw_dir\"] = DATASET_RAW_DIR\n",
    "args[\"processed_dir\"] = DATASET_PROCESSED_DIR\n",
    "dataset = MPDataset(DATASET_DIR, args)\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "def split_dataset(dataset: Dataset,\n",
    "                  train_ratio=0.6,\n",
    "                  test_ratio=0.2,\n",
    "                  val_ratio=0.2,\n",
    "                  lengths=None) -> list[Dataset]:\n",
    "    \"\"\"\n",
    "    Split the dataset by ratio or length. \\n\n",
    "    Provide lengths (e.g. lengths=[6000,2000,2000]) will split by length provided. \\n\n",
    "    Otherwise split by ratio\n",
    "    \"\"\"\n",
    "    dataset_len = dataset.len()\n",
    "\n",
    "    lengths_mode = True if (isinstance(lengths, list)\n",
    "                            and len(lengths) == 3) else False\n",
    "\n",
    "    if lengths_mode is False and train_ratio + test_ratio + val_ratio != 1.0:\n",
    "        raise ValueError(\"The total ratio of split dataset is not 1.0.\")\n",
    "\n",
    "    train_len = int(lengths[0] if lengths_mode else dataset_len * train_ratio)\n",
    "    test_len = int(lengths[1] if lengths_mode else dataset_len * test_ratio)\n",
    "    val_len = int(lengths[2] if lengths_mode else dataset_len * val_ratio)\n",
    "\n",
    "    idx = list(range(dataset_len))\n",
    "\n",
    "    # random.seed(1) #* for result reproduction\n",
    "    random.shuffle(idx)\n",
    "\n",
    "    train_dataset = dataset.index_select(idx[:train_len])\n",
    "    test_dataset = dataset.index_select(idx[train_len:train_len + test_len])\n",
    "    validation_dataset = dataset.index_select(\n",
    "        idx[train_len + test_len:train_len + test_len + val_len])\n",
    "\n",
    "    return train_dataset, test_dataset, validation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[0, 1, 2, 3, 4][0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "5\n",
      "-6\n",
      "22\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# Import math library\n",
    "import math\n",
    "\n",
    "# Round numbers down to the nearest integer\n",
    "print(math.floor(0.6))\n",
    "print(math.floor(1.4))\n",
    "print(math.floor(5.3))\n",
    "print(math.floor(-5.3))\n",
    "print(math.floor(22.6))\n",
    "print(math.floor(10.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No module named 'phonopy'\n",
      "No module named 'phonopy'\n",
      "Downloading raw dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e6213d694b546079b10a20bdafc833c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Retrieving SummaryDoc documents:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Processing dataset: 100%|██████████| 100/100 [00:00<00:00, 1093.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_min  -1.2514026165008545\n",
      "data_max  1.5960882902145386\n",
      "MPDataset(70) MPDataset(15) MPDataset(15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from dataset import *\n",
    "from args import *\n",
    "\n",
    "train_set, test_set, val_set = make_dataset(args)\n",
    "print(train_set, test_set, val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPDataset(70) 32\n",
      "No module named 'phonopy'No module named 'phonopy'\n",
      "\n",
      "No module named 'phonopy'\n",
      "No module named 'phonopy'\n",
      "No module named 'phonopy'\n",
      "No module named 'phonopy'\n",
      "No module named 'phonopy'\n",
      "No module named 'phonopy'\n",
      "No module named 'phonopy'\n",
      "No module named 'phonopy'\n",
      "DataBatch(edge_index=[2, 660], edge_attr=[660, 1], x=[230, 1], y=[32], batch=[230], ptr=[33]) 32 tensor([ 0,  0,  0,  0,  1,  1,  1,  1,  2,  2,  2,  2,  3,  3,  3,  3,  4,  4,\n",
      "         4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  8,  8,  8,  8,\n",
      "         9,  9,  9,  9, 10, 10, 10, 10, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 14, 14, 14, 14, 15, 15,\n",
      "        15, 15, 16, 16, 16, 16, 17, 17, 17, 17, 18, 18, 18, 18, 19, 19, 19, 19,\n",
      "        19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 21, 21, 21,\n",
      "        21, 22, 22, 22, 22, 23, 23, 23, 23, 24, 24, 24, 24, 24, 24, 24, 24, 24,\n",
      "        24, 24, 24, 24, 24, 24, 25, 25, 25, 25, 26, 26, 26, 26, 27, 27, 27, 27,\n",
      "        28, 28, 28, 28, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 30, 30, 30, 30,\n",
      "        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31])\n",
      "DataBatch(edge_index=[2, 1334], edge_attr=[1334, 1], x=[305, 1], y=[32], batch=[305], ptr=[33]) 32 tensor([ 0,  0,  0,  0,  1,  1,  1,  1,  2,  2,  2,  2,  3,  3,  3,  3,  3,  3,\n",
      "         3,  3,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  5,  5,  5,  5,\n",
      "         6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "         6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "         6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "         6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  7,  7,  7,\n",
      "         7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,\n",
      "         8,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9, 10, 10, 10, 10, 11, 11, 11,\n",
      "        11, 12, 12, 12, 12, 13, 13, 13, 13, 14, 14, 14, 14, 15, 15, 15, 15, 16,\n",
      "        16, 16, 16, 17, 17, 17, 17, 18, 18, 18, 18, 19, 19, 19, 19, 20, 20, 20,\n",
      "        20, 21, 21, 21, 21, 22, 22, 22, 22, 23, 23, 23, 23, 24, 24, 24, 24, 25,\n",
      "        25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
      "        25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
      "        25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
      "        25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
      "        25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 26, 26, 26, 26, 27, 27, 27,\n",
      "        27, 28, 28, 28, 28, 29, 29, 29, 29, 30, 30, 30, 30, 31, 31, 31, 31])\n",
      "DataBatch(edge_index=[2, 36], edge_attr=[36, 1], x=[24, 1], y=[6], batch=[24], ptr=[7]) 6 tensor([0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "\n",
    "def make_data_loader() -> list[Dataset]:\n",
    "    train_set, test_set, val_set = make_dataset(args)\n",
    "\n",
    "    train_data_loader = DataLoader(\n",
    "        train_set,\n",
    "        batch_size=args[\"batch_size\"],\n",
    "        shuffle=args[\"data_loader_shuffle\"],\n",
    "        num_workers=args[\"num_workers\"],\n",
    "        #  generator=g\n",
    "    )\n",
    "\n",
    "    test_data_loader = DataLoader(\n",
    "        test_set,\n",
    "        batch_size=args[\"batch_size\"],\n",
    "        shuffle=args[\"data_loader_shuffle\"],\n",
    "        num_workers=args[\"num_workers\"],\n",
    "        #  generator=g\n",
    "    )\n",
    "\n",
    "    val_data_loader = DataLoader(\n",
    "        val_set,\n",
    "        batch_size=args[\"batch_size\"],\n",
    "        shuffle=args[\"data_loader_shuffle\"],\n",
    "        num_workers=args[\"num_workers\"],\n",
    "        # generator=g\n",
    "    )\n",
    "\n",
    "    return train_data_loader, test_data_loader, val_data_loader\n",
    "\n",
    "\n",
    "train_data_loader, test_data_loader, val_data_loader = make_data_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creat a graph model\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "\n",
    "class GCNConvNetwork(torch.nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, 16)\n",
    "        self.conv2 = GCNConv(16, dataset.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = args[\"epochs\"]\n",
    "for epoch in range(1, epochs):\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OneHot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 0, 0],\n",
      "        [0, 1, 0],\n",
      "        [1, 0, 0],\n",
      "        [0, 1, 0]]) tensor([[1., 2., 3.],\n",
      "        [1., 2., 3.],\n",
      "        [1., 2., 3.],\n",
      "        [1., 2., 3.]])\n",
      "tensor([[1., 0., 0., 1., 2., 3.],\n",
      "        [0., 1., 0., 1., 2., 3.],\n",
      "        [1., 0., 0., 1., 2., 3.],\n",
      "        [0., 1., 0., 1., 2., 3.]])\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import torch\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "# Generate a one-hot encoding dictionary\n",
    "# Note: This dictionary is for the WHOLE data collection.\n",
    "def gen_one_hot_dict(symbols):\n",
    "    \"\"\"\n",
    "    symbols: a list of unique symbols, eg. ['C','N','C','P']\n",
    "             This list should have ALL the elements in the collection of data\n",
    "    \"\"\"\n",
    "    # Covert symbols to numeric type list\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    symbols_num_list = le.fit_transform(symbols)\n",
    "    # print(symbols_num_list)\n",
    "    symbols = torch.as_tensor(symbols_num_list)\n",
    "\n",
    "    symbols = torch.unique(symbols)\n",
    "    # generate one-hot encoding dictionary\n",
    "    numS = symbols.size(dim=0)\n",
    "    code = [0] * numS\n",
    "    atom_type_dict = {}\n",
    "    pos = 0\n",
    "    for atom in symbols:\n",
    "        a_code = copy.deepcopy(code)\n",
    "        a_code[pos] = 1\n",
    "        atom_type_dict[atom.item()] = a_code\n",
    "        pos += 1\n",
    "    return atom_type_dict\n",
    "\n",
    "\n",
    "def atom_list_2_one_hot(atom_list, atom_type_dict):\n",
    "    \"\"\"\n",
    "    atom_list: a tensor of atom symbols. eg. ['C','N','C','N']\n",
    "    This function returns a list of one-hot encoded atom types\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Covert atom_list to numeric type list\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    atom_num_list = le.fit_transform(atom_list)\n",
    "    atom_num_list = torch.as_tensor(atom_num_list)\n",
    "\n",
    "    # one-hot encoding\n",
    "    atom_type_list = []\n",
    "    for atom in atom_num_list:\n",
    "        acode = atom_type_dict[atom.item()]\n",
    "        atom_type_list.append(acode)\n",
    "    atom_type_list = torch.tensor(atom_type_list)\n",
    "    return atom_type_list\n",
    "\n",
    "\n",
    "dict = gen_one_hot_dict([\"C\", \"N\", \"C\", \"P\"])\n",
    "list = atom_list_2_one_hot([\"C\", \"N\", \"C\", \"N\"], dict)\n",
    "coordinates = torch.Tensor(\n",
    "    [\n",
    "        [1.0, 2.0, 3.0],\n",
    "        [1.0, 2.0, 3.0],\n",
    "        [1.0, 2.0, 3.0],\n",
    "        [1.0, 2.0, 3.0],\n",
    "    ]\n",
    ")\n",
    "print(list, coordinates)\n",
    "x = torch.cat([list, coordinates], dim=1)\n",
    "print(x)\n",
    "print(x.dtype)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tqdm\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from args import *\n",
    "from dataset import *\n",
    "from utils import *\n",
    "from model import CEALNetwork, GCNNetwork, load_model\n",
    "\n",
    "import os.path as osp\n",
    "from train import make_data_loader, train_step, test_evaluations\n",
    "\n",
    "from module.madgap import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "train_dataset, validation_dataset, test_dataset = make_dataset()\n",
    "batch_size = 100\n",
    "train_loader, val_loader, test_loader = make_data_loader(train_dataset, validation_dataset, test_dataset, batch_size=batch_size)\n",
    "\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find features of high prediction error compounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.spatial.distance import cdist\n",
    "# def data_stat(data_list):\n",
    "#     avg_num_edges = 0\n",
    "#     avg_num_nodes = 0\n",
    "#     for i,d in enumerate(data_list):\n",
    "#         avg_num_edges+=d.edge_index.shape[-1]\n",
    "#         avg_num_nodes+=d.x.shape[0]\n",
    "#     total_graphs = i+1\n",
    "#     return avg_num_nodes/total_graphs,avg_num_edges/total_graphs\n",
    "\n",
    "# def ase_data_stat(ase_data_list):\n",
    "#     avg_volume = 0.0\n",
    "#     avg_radius = 0.0\n",
    "    \n",
    "#     for i,d in enumerate(ase_data_list):\n",
    "#         avg_volume+=d.get_volume()\n",
    "        \n",
    "#         positions = d.get_positions()\n",
    "#         dist_matrix = cdist(positions, positions)\n",
    "#         max_distances = dist_matrix.max(axis=1)\n",
    "#         radius = max_distances.max() / 2.0\n",
    "#         avg_radius+=radius\n",
    "        \n",
    "#     total = i+1\n",
    "#     return avg_volume/total,avg_radius/total\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     high_pred_compounds = []\n",
    "#     for i, batch_data in enumerate(test_loader):\n",
    "#         batch_data.to(get_device())\n",
    "#         out = model(batch_data, node_embedding=False)\n",
    "    \n",
    "#         # reverse data scale\n",
    "#         min, max = get_data_scale(args)\n",
    "#         res_out = reverse_min_max_scalar_1d(out, min, max)\n",
    "#         res_y =  reverse_min_max_scalar_1d( batch_data.y, min, max)\n",
    "        \n",
    "#         # get high prediction error compounds\n",
    "#         error =  (res_out.squeeze() - res_y).abs()\n",
    "#         index = torch.where(error > 1.0)[0]\n",
    "#         compounds = [{'mid':data.mid,'idx':data.idx} for _,data in enumerate(batch_data[index])]\n",
    "#         high_pred_compounds.extend(compounds)\n",
    "        \n",
    "#         # high_pred_compounds.append()\n",
    "        \n",
    "#         torch.cuda.empty_cache()\n",
    "        \n",
    "#     high_pred_compounds.sort(key=lambda x: int(x['idx']))\n",
    "    \n",
    "    \n",
    "#     all_dataset = MPDataset(args)\n",
    "#     dataset_stat = data_stat(all_dataset)\n",
    "#     print(f\"dataset.             avg_num_nodes:{round(dataset_stat[0],4)}, avg_num_edges:{round(dataset_stat[1],4)}\")\n",
    "#     # read all ase file from the whole dataset\n",
    "#     whole_pred_ase = []\n",
    "#     for i,d in enumerate(all_dataset):\n",
    "#         path = osp.join(args[\"dataset_raw_dir\"],f\"CONFIG_{int(d.idx)}.poscar\")\n",
    "#         compound = ase_read(path, format=\"vasp\")\n",
    "#         whole_pred_ase.append(compound)\n",
    "#     dataset_ase_stat = ase_data_stat(whole_pred_ase)\n",
    "#     print(f\"dataset.               avg_volume:{round(dataset_ase_stat[0],4)}, avg_radius:{round(dataset_ase_stat[1],4)}\")\n",
    "\n",
    "#     high_pred_data = [all_dataset[int(d['idx'])-1] for i,d in enumerate(high_pred_compounds)]\n",
    "#     high_stat = data_stat(high_pred_data)\n",
    "#     print(f\"high_pred_compounds. avg_num_nodes:{round(high_stat[0],4)}, avg_num_edges:{round(high_stat[1],4)}\")\n",
    "#     # read all ase file from high pred error compounds\n",
    "#     high_pred_ase = []\n",
    "#     for i,d in enumerate(high_pred_data):\n",
    "#         path = osp.join(args[\"dataset_raw_dir\"],f\"CONFIG_{int(d.idx)}.poscar\")\n",
    "#         compound = ase_read(path, format=\"vasp\")\n",
    "#         high_pred_ase.append(compound)\n",
    "#     high_ase_stat = ase_data_stat(high_pred_ase)\n",
    "#     print(f\"high_pred_compounds.  avg_volume:{round(high_ase_stat[0],4)}, avg_radius:{round(high_ase_stat[1],4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset.             avg_num_nodes:28.3963, avg_num_edges:164.8967\n",
      "dataset.               avg_volume:710.3466, avg_radius:6.4753\n",
      "high_pred_compounds. avg_num_nodes:17.8939, avg_num_edges:94.5455\n",
      "high_pred_compounds.  avg_volume:1409.8142, avg_radius:6.3831\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "\n",
    "def data_stat(data_list):\n",
    "    avg_num_edges = 0\n",
    "    avg_num_nodes = 0\n",
    "    for i, d in enumerate(data_list):\n",
    "        avg_num_edges += d.edge_index.shape[-1]\n",
    "        avg_num_nodes += d.x.shape[0]\n",
    "    total_graphs = i + 1\n",
    "    return avg_num_nodes / total_graphs, avg_num_edges / total_graphs\n",
    "\n",
    "\n",
    "def ase_data_stat(ase_data_list):\n",
    "    avg_volume = 0.0\n",
    "    avg_radius = 0.0\n",
    "\n",
    "    for i, d in enumerate(ase_data_list):\n",
    "        avg_volume += d.get_volume()\n",
    "\n",
    "        positions = d.get_positions()\n",
    "        dist_matrix = cdist(positions, positions)\n",
    "        max_distances = dist_matrix.max(axis=1)\n",
    "        radius = max_distances.max() / 2.0\n",
    "        avg_radius += radius\n",
    "\n",
    "    total = i + 1\n",
    "    return avg_volume / total, avg_radius / total\n",
    "\n",
    "\n",
    "def get_high_pred_error_stats(dataloader, model_path, threshold=0.5):\n",
    "\n",
    "    high_pred_compounds = []\n",
    "\n",
    "    model, _ = load_model(model_path)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch_data in enumerate(dataloader):\n",
    "            batch_data.to(get_device())\n",
    "            out = model(batch_data, node_embedding=False)\n",
    "\n",
    "            # reverse data scale\n",
    "            min, max = get_data_scale(args)\n",
    "            res_out = reverse_min_max_scalar_1d(out, min, max)\n",
    "            res_y = reverse_min_max_scalar_1d(batch_data.y, min, max)\n",
    "\n",
    "            # get high prediction error compounds\n",
    "            error = (res_out.squeeze() - res_y).abs()\n",
    "            index = torch.where(error > threshold)[0]\n",
    "            compounds = [{\"mid\": data.mid, \"idx\": data.idx} for _, data in enumerate(batch_data[index])]\n",
    "            high_pred_compounds.extend(compounds)\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    high_pred_compounds.sort(key=lambda x: int(x[\"idx\"]))\n",
    "\n",
    "    all_dataset = MPDataset(args)\n",
    "    dataset_stat = data_stat(all_dataset)\n",
    "    print(f\"dataset.             avg_num_nodes:{round(dataset_stat[0],4)}, avg_num_edges:{round(dataset_stat[1],4)}\")\n",
    "    # read all ase file from the whole dataset\n",
    "    whole_pred_ase = []\n",
    "    for i, d in enumerate(all_dataset):\n",
    "        path = osp.join(args[\"dataset_raw_dir\"], f\"CONFIG_{int(d.idx)}.poscar\")\n",
    "        compound = ase_read(path, format=\"vasp\")\n",
    "        whole_pred_ase.append(compound)\n",
    "    dataset_ase_stat = ase_data_stat(whole_pred_ase)\n",
    "    print(f\"dataset.               avg_volume:{round(dataset_ase_stat[0],4)}, avg_radius:{round(dataset_ase_stat[1],4)}\")\n",
    "\n",
    "    high_pred_data = [all_dataset[int(d[\"idx\"]) - 1] for i, d in enumerate(high_pred_compounds)]\n",
    "    high_stat = data_stat(high_pred_data)\n",
    "    print(f\"high_pred_compounds. avg_num_nodes:{round(high_stat[0],4)}, avg_num_edges:{round(high_stat[1],4)}\")\n",
    "    # read all ase file from high pred error compounds\n",
    "    high_pred_ase = []\n",
    "    for i, d in enumerate(high_pred_data):\n",
    "        path = osp.join(args[\"dataset_raw_dir\"], f\"CONFIG_{int(d.idx)}.poscar\")\n",
    "        compound = ase_read(path, format=\"vasp\")\n",
    "        high_pred_ase.append(compound)\n",
    "    high_ase_stat = ase_data_stat(high_pred_ase)\n",
    "    print(f\"high_pred_compounds.  avg_volume:{round(high_ase_stat[0],4)}, avg_radius:{round(high_ase_stat[1],4)}\")\n",
    "\n",
    "\n",
    "get_high_pred_error_stats(test_loader, osp.join(args[\"result_path\"], \"CEAL/1717229364248831\"), threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset.             avg_num_nodes:28.3963, avg_num_edges:353.3166\n",
      "dataset.               avg_volume:710.3466, avg_radius:6.4753\n"
     ]
    }
   ],
   "source": [
    "all_dataset = MPDataset(args)\n",
    "dataset_stat = data_stat(all_dataset)\n",
    "print(f\"dataset.             avg_num_nodes:{round(dataset_stat[0],4)}, avg_num_edges:{round(dataset_stat[1],4)}\")\n",
    "# read all ase file from the whole dataset\n",
    "whole_pred_ase = []\n",
    "for i, d in enumerate(all_dataset):\n",
    "    path = osp.join(args[\"dataset_raw_dir\"], f\"CONFIG_{int(d.idx)}.poscar\")\n",
    "    compound = ase_read(path, format=\"vasp\")\n",
    "    whole_pred_ase.append(compound)\n",
    "dataset_ase_stat = ase_data_stat(whole_pred_ase)\n",
    "print(f\"dataset.               avg_volume:{round(dataset_ase_stat[0],4)}, avg_radius:{round(dataset_ase_stat[1],4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the MAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 6272/6272 [00:59<00:00, 105.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAD: 0.22039625318877537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def calculate_MAD(dataset, model_path, device=get_device()):\n",
    "    mad_total = 0.0\n",
    "    total_data_size = len(dataset)\n",
    "\n",
    "    predict_epochs = total_data_size\n",
    "    pbar = tqdm(total=predict_epochs)\n",
    "    pbar.set_description(\"Progress\")\n",
    "\n",
    "    model, _ = load_model(model_path)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataset):\n",
    "            data.to(device)\n",
    "            node_embeddings = model(data, node_embedding=True)\n",
    "\n",
    "            in_arr = node_embeddings.cpu().detach().numpy()\n",
    "\n",
    "            num_nodes = data.x.shape[0]\n",
    "            adj = torch.zeros((num_nodes, num_nodes))\n",
    "            adj[data.edge_index[0], data.edge_index[1]] = 1\n",
    "            mask_arr = adj.numpy()\n",
    "\n",
    "            mad_single = mad_value(in_arr, mask_arr)\n",
    "            mad_total += mad_single\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            pbar.update(1)\n",
    "        pbar.close()\n",
    "\n",
    "    return mad_total / total_data_size\n",
    "\n",
    "print(\"MAD:\", calculate_MAD(test_dataset, osp.join(args[\"result_path\"], \"CEAL/1716963786873904\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the reachable nodes and the shared reachable nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./dataset.max_cutoff==5.0/processed\n"
     ]
    }
   ],
   "source": [
    "print(DATASET_PROCESSED_DIR)\n",
    "\n",
    "# def reachable_nodes(num_nodes, edge_index, num_layers=1, ret_mat=False, device=get_device()):\n",
    "#     adj = torch.tensor(np.eye(num_nodes), dtype=int).to(device)\n",
    "#     adj[edge_index[0], edge_index[1]] = 1\n",
    "#     if num_layers == 1:\n",
    "#         return torch.sum(adj, dim=0).to(device) if ret_mat is False else adj\n",
    "\n",
    "#     res_adj = adj\n",
    "\n",
    "#     for i in range(0, num_layers - 1):\n",
    "#         columns = []\n",
    "#         for column in range(0, num_nodes):\n",
    "#             # get column data\n",
    "#             data = torch.where(res_adj[column] > 0)[0].to(device)\n",
    "#             # get all node index data\n",
    "#             data = torch.where(torch.sum(adj[data], dim=0) > 0)[0].to(device)\n",
    "\n",
    "#             line = torch.zeros(num_nodes, dtype=int).to(device)\n",
    "#             line[data] = 1\n",
    "#             columns.append(line)\n",
    "#         res_adj = torch.stack(columns).to(device)\n",
    "\n",
    "#     return torch.sum(res_adj, dim=0).to(device) if ret_mat is False else res_adj\n",
    "\n",
    "\n",
    "def reachable_nodes_mat(num_nodes, edge_index, num_layers=1, device=get_device()):\n",
    "    adj = torch.tensor(np.eye(num_nodes), dtype=float).to(device)\n",
    "    adj[edge_index[0], edge_index[1]] = 1\n",
    "    if num_layers == 1:\n",
    "        adj = adj.fill_diagonal_(0.0)\n",
    "        return adj\n",
    "\n",
    "    res_adj = adj.clone()\n",
    "    for i in range(0, num_layers - 1):\n",
    "        res_adj = res_adj @ adj\n",
    "\n",
    "    res_adj = res_adj.fill_diagonal_(0.0)\n",
    "\n",
    "    res_adj_idx = torch.where(res_adj >= 1.0)\n",
    "    res_adj[res_adj_idx] = 1.0\n",
    "\n",
    "    return res_adj\n",
    "\n",
    "\n",
    "# def get_avg_reachable_nodes(total_graphs, dataloader, num_layers, device=get_device()):\n",
    "#     predict_epochs = math.ceil(total_graphs / batch_size)\n",
    "#     pbar = tqdm(total=predict_epochs)\n",
    "#     pbar.set_description(\"Progress\")\n",
    "\n",
    "#     total_num_nodes = 0\n",
    "#     total_reachable_nodes = 0\n",
    "\n",
    "#     total_batch_size = 0\n",
    "#     total_shared_nodes = 0\n",
    "\n",
    "#     for i, batch_data in enumerate(dataloader):\n",
    "#         num_nodes = batch_data.num_nodes\n",
    "#         edge_index = batch_data.edge_index.to(device)\n",
    "\n",
    "#         # reachable matrix\n",
    "#         reachable_mat = reachable_nodes_mat(num_nodes, edge_index, num_layers, device=device)\n",
    "#         reachable_nodes = torch.sum(reachable_mat)\n",
    "#         total_reachable_nodes += reachable_nodes\n",
    "\n",
    "#         # shared reachable matrix\n",
    "#         shared_mat = reachable_mat @ reachable_mat\n",
    "#         shared_mat = shared_mat.fill_diagonal_(0)\n",
    "#         shared_nodes = torch.sum(shared_mat)\n",
    "#         # solve avg. shared reachable nodes per node for this batch\n",
    "#         shared_nodes = shared_nodes / (num_nodes * (num_nodes + 1))\n",
    "#         total_shared_nodes += shared_nodes\n",
    "\n",
    "#         total_batch_size += 1\n",
    "#         total_num_nodes += num_nodes\n",
    "\n",
    "#         torch.cuda.empty_cache()\n",
    "#         pbar.update(1)\n",
    "#     pbar.close()\n",
    "\n",
    "#     avg_reachable_nodes = (total_reachable_nodes / total_num_nodes).item()\n",
    "#     avg_shared_reachable_nodes = (total_shared_nodes / total_batch_size).item()\n",
    "#     avg_nodes_on_graph = total_num_nodes / total_graphs\n",
    "#     print(f\"Layer {num_layers}\")\n",
    "#     print(\"Average reachable nodes:\", round(avg_reachable_nodes, 4))\n",
    "#     print(\"Average shared reachable nodes:\", round(avg_shared_reachable_nodes, 4))\n",
    "#     print(\"Average nodes on a graph:\", round(avg_nodes_on_graph, 4))\n",
    "#     print(f\"=================================\")\n",
    "\n",
    "#     return avg_reachable_nodes, avg_nodes_on_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_reachable_nodes(dataset, num_layers, device=get_device()):\n",
    "    total_num_nodes = 0\n",
    "    total_reachable_nodes = 0\n",
    "\n",
    "    total_data_size = len(dataset)\n",
    "    total_shared_nodes = 0\n",
    "    \n",
    "    predict_epochs = total_data_size\n",
    "    pbar = tqdm(total=predict_epochs)\n",
    "    pbar.set_description(\"Progress\")\n",
    "\n",
    "    for i, data in enumerate(dataset):\n",
    "        num_nodes = data.num_nodes\n",
    "        edge_index = data.edge_index.to(device)\n",
    "\n",
    "        # reachable matrix\n",
    "        reachable_mat = reachable_nodes_mat(num_nodes, edge_index, num_layers, device=device)\n",
    "        reachable_nodes = torch.sum(reachable_mat)\n",
    "        total_reachable_nodes += reachable_nodes\n",
    "\n",
    "        # shared reachable matrix\n",
    "        shared_mat = reachable_mat @ reachable_mat\n",
    "        shared_mat = shared_mat.fill_diagonal_(0)\n",
    "        shared_nodes = torch.sum(shared_mat)\n",
    "        # solve avg. shared reachable nodes per node for this batch\n",
    "        shared_nodes = shared_nodes / (num_nodes * (num_nodes + 1))\n",
    "        total_shared_nodes += shared_nodes\n",
    "\n",
    "        total_num_nodes += num_nodes\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "\n",
    "    avg_reachable_nodes = (total_reachable_nodes / total_num_nodes).item()\n",
    "    avg_shared_reachable_nodes = (total_shared_nodes / total_data_size).item()\n",
    "    avg_nodes_on_graph = total_num_nodes / total_data_size\n",
    "    print(f\"Layer {num_layers}\")\n",
    "    print(\"Average reachable nodes:\", round(avg_reachable_nodes, 4))\n",
    "    print(\"Average shared reachable nodes:\", round(avg_shared_reachable_nodes, 4))\n",
    "    print(\"Average nodes on a graph:\", round(avg_nodes_on_graph, 4))\n",
    "    print(f\"=================================\")\n",
    "\n",
    "    return avg_reachable_nodes, avg_shared_reachable_nodes, avg_nodes_on_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_avg_reachable_nodes(train_dataset, 1)\n",
    "get_avg_reachable_nodes(train_dataset, 2)\n",
    "get_avg_reachable_nodes(train_dataset, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edge_index = torch.tensor(\n",
    "#     [\n",
    "#         [0, 0, 0, 0, 1, 1, 1, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 6, 6, 6, 7, 7, 8, 8],\n",
    "#         [0, 1, 2, 3, 0, 1, 8, 0, 2, 0, 3, 4, 5, 3, 4, 5, 6, 3, 4, 5, 4, 6, 7, 6, 7, 1, 8],\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# reachable_mat = reachable_nodes_mat(9, edge_index, 10, device=get_device())\n",
    "# print(reachable_mat)\n",
    "# # reachable_nodes = torch.sum(reachable_mat)\n",
    "# # print(reachable_nodes)\n",
    "\n",
    "# shared_mat = reachable_mat @ reachable_mat\n",
    "# shared_mat = shared_mat.fill_diagonal_(0)\n",
    "# shared_nodes = torch.sum(shared_mat)\n",
    "# print(shared_mat)\n",
    "# print((shared_nodes / (7 * (7 + 1))))\n",
    "\n",
    "# # reachable_mat_idx = torch.where(reachable_mat >= 1.0)\n",
    "# # reachable_mat[shared_mat_idx] = 1.0\n",
    "\n",
    "# # reachable_mat = reachable_nodes(9, edge_index, 2, ret_mat=True, device=get_device())\n",
    "# # print(reachable_mat)\n",
    "\n",
    "# # reachable_mat = reachable_mat.float()\n",
    "# # mat = (reachable_mat @ reachable_mat)\n",
    "# # print(mat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "material",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

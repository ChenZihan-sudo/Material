# All Tune configurations can be inherited from the original config.yml.
# The original config.yml settings that overlap with the Tune configurations
# will be replaced by the Tune configurations.

# ===============Template===============
# value: # any value you want to tune in here
#   tune: [method, *params]
#
# Paramters
# tune   => tags for recognization
# method => search methods in tune or customed by the user
# params => search params
# ======================================

# Tuning:
#   data_loader:
#     batch_size:
#       tune: [choice, [100]] # [qrandint, 600, 800, 100]

Models:
  ChemGNN:
    conv_params:
    #   aggregators: [sum, mean, min, max, std]
    #   scalers: [identity]
    #   towers: 1
    #   pre_layers: 1
    #   post_layers: 1
    #   divide_input: false
      aggMLP: 
        tune: [choice, [True, False]]
      aggMLP_factor: 
        tune: [choice, [0.4, 0.6, 0.8, 1.0]]
    pre_fc_dim:
      tune: [fc_dim, [0, 1, 2, 3], [50, 100, 150, 200]] # choice, num_layers and dim
    num_layers:
      tune: [choice, [1, 2]]
    conv_out_dim:
      tune: [choice, [100, 150]]
    post_fc_dim:
      tune: [fc_dim, [0, 1, 2, 3], [50, 100, 150, 200]] # choice, num_layers and dim
    # dropout_rate: 0.0
    pool:
      tune: [choice, ["global_mean_pool"]]
    learning_rate:
      tune: [loguniform, 0.005, 0.05]
    # optimizer:
    #   name: AdamW
    #   params:
    scheduler:
      # name: ReduceLROnPlateau
      params:
        # mode: min
        factor: 
          tune: [choice, [0.5, 0.6, 0.7, 0.8, 0.9]]
        patience: 
          tune: [choice, [5, 10, 15, 20]]
        # min_lr: 0.00000001
